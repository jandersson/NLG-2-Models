\documentclass[ai15_group61_report.tex]{subfiles}
\begin{document}

%Again \cite{Reiter:2000:BNL:331955} explains that one approach to generating text is not through NLG, rather by a technique known as mail-merge whereby a template is created and the appropriate data is filled in. In designing the experiment for the "Inferred Grammar" model, an approach similar to mail-merge was carried out.

In \cite{Ratnaparkhi00} three methods for text generation are examined. One of the models examined makes use of n-gram models where phrases are generated from left to right. Again, \cite{Ratnaparkhi00} explores methods for generating a semantic structure based on those found in the corpus. A model used in \cite{Ratnaparkhi00} creates an automatic attribute ordering, connecting words with semantics. The models IGM and SM take inspiration from this approach. 

Several textbooks explore the topic of language modelling using n-grams and part of speech tags. Works such as \cite{Jurafsky2000}, \cite{RussellNorvigAIBook3rd}, and \cite{Chen98anempirical} provide excellent tutorials to language modelling and were used extensively in this project. A discussion of n-gram order by \cite{RussellNorvigAIBook3rd} demonstrates that unigrams are a poor representation of the english language and that bigrams and trigrams provide better approximations. Unigrams perform poorly due to the loss of any ordering based on previous words or tokens \cite{RussellNorvigAIBook3rd}. An error in our experiment construction led to the inferred grammar model behaving similarly to this bag of words model.

The Natural Language Tool Kit (NLTK) boosted development productivity through a suite of tools such as probability distribution classes, n-gram decomposition functions, corpora and corpora importing utilities, and speech tagging. We extensively referenced \cite{NLTKBook09} for guidance in use of the NLTK. 

Continuing on the topic of tutorials; the study of smoothing techniques, perplexity, and n-gram models provided by \cite{Jurafsky2000} were referenced in verifying the output produced by our text generation techniques. For example, our output produced by quad-gram models were suspiciously close to human authored text. Indeed, \cite{Jurafsky2000} states that the N-gram probability matrices become sparse as order increases, diminishing the set of possible continuations to approximately one. This effectively generates sentences which are found verbatim in the corpus. As such, the discussion in \cite{Jurafsky2000} led to our decision to implement trigram models. 

%Further, perplexity scores are a quantitative correlation to readability, writes Jurafsky et al.  

Smoothing has been shown, empirically, to improve the prediction accuracy of N-gram models by \cite{chen-smoothing}. We referenced this work in determining which smoothing techniques to implement in our experiment. Discussion of the employed smoothing techniques are found in the section on smoothing.

%By making the distributions within the N-gram model more uniform, pushing down high probabilitities and increasing low, the model can account for zero probabilities. It is further argued by \cite{chen-smoothing} that Add-One or Laplace smoothing performs poorly, due to shaving off too much probability from the higher count N-grams.  

\end{document} 