\documentclass[ai15_group61_report.tex]{subfiles}
\begin{document}

%Again \cite{Reiter:2000:BNL:331955} explains that one approach to generating text is not through NLG, rather by a technique known as mail-merge whereby a template is created and the appropriate data is filled in. In designing the experiment for the "Inferred Grammar" model, an approach similar to mail-merge was carried out.

In \cite{Ratnaparkhi00} Ratnaparkhi examines three models for text generation. One of the models examined makes use of N-gram models where phrases are generated from left to right. Ratnaparkhi further explores methods for generating a semantic structure based on those found in a corpus. A model used by Ratnaparkhi creates an automatic attribute ordering, connecting words with semantics. The IGM and SM models, detailed in section~\ref{sec:method}, are inspired by this approach. 

Several textbooks explore the topic of language modeling using N-grams and part of speech tags. Works by authors such as Jurafsky and Martin\cite{Jurafsky2000}, Russell and Norvig\cite{RussellNorvigAIBook3rd}, and Chen\cite{chen-smoothing} provide excellent tutorials to language modeling and were referenced extensively over the course of this project. A discussion of N-gram order by Russell and Norvig demonstrates that unigrams are a poor representation of the English language and that bigrams and trigrams provide better approximations. Unigrams perform poorly due to the loss of any ordering based on previous words or tokens. An error in our experiment construction led to the Inferred Grammar model behaving similarly to this bag of words model.

The Natural Language Tool Kit (NLTK) boosted development productivity through a suite of tools such as probability distribution classes, N-gram decomposition functions, corpora and corpora importing utilities, and speech tagging. We extensively referenced \cite{NLTKBook09} for guidance in use of the NLTK. 

Continuing on the topic of tutorials; the study of smoothing techniques, perplexity, and N-gram models provided by Jurafsky and Martin\cite{Jurafsky2000} were referenced in verifying the output produced by our text generation techniques. For example, our output produced by quad-gram models were suspiciously close to human authored text. Indeed the authors state that the N-gram probability matrices become sparse as order increases, diminishing the set of possible continuations to approximately one. This effectively generates sentences which are found verbatim in the corpus. This discussion influenced our decision to use trigrams in our models. 

%Further, perplexity scores are a quantitative correlation to readability, writes Jurafsky et al.  

Smoothing has been shown, empirically, to improve the prediction accuracy of N-gram models\cite{chen-smoothing}. We referenced this work when choosing the smoothing techniques to evaluate with our models, as discussed in section~\ref{subsec:smoothing}.

%By making the distributions within the N-gram model more uniform, pushing down high probabilitities and increasing low, the model can account for zero probabilities. It is further argued by \cite{chen-smoothing} that Add-One or Laplace smoothing performs poorly, due to shaving off too much probability from the higher count N-grams.  

\end{document} 