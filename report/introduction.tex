\documentclass[ai15_group61_report.tex]{subfiles}
\begin{document}
Language models are a staple in many domains including speech recognition, optical character
recognition, handwriting recognition, machine translation, and spelling correction. The dominant technology in language modeling is n-gram models, which are straightforward to construct except for the issue of smoothing, a technique used to better estimate probabilities when there is insufficient data to estimate probabilities accurately\textbf{[Chen-Goodman]}. Many different techniques have been proposed for smoothing n-gram models. In this project we show how different smoothing techniques work and how they will affect the sentences generated.

In the following sections we will describe the difference in the models generated and how including grammar affects
\end{document}