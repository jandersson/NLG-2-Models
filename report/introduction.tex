\documentclass[ai15_group61_report.tex]{subfiles}
\begin{document}

In general, according to \cite{Reiter:2000:BNL:331955}, NLG systems are constructed after performing requirements analysis whereby the goals of the NLG system are defined. Indeed, the project team designed the two NLG approaches after performing such a requirements analysis. The derived motivation is to use a statistical approach to generating sentences with a sound grammatical structure without having to hand-craft any grammatical rules. This approach is similar to \cite{Ratnaparkhi00}.

We approach the problem in two different ways, both of them making use of a corpus annotated with Part-of-Speech tags (POS-tags). The first model, we chose to call the Semantic Model (SM henceforth), because it disambiguates the words in the sentences by building grams out of (word, POS-tag) tuples. This gives the model a larger vocabulary where each word has a more specific meaning. The second model we call the Inferred Grammal Model (IGM henceforth). The IGM is different in that is uses two separate n-gram models, one for generating the grammatical structure of a sentence, and another one for filling that structure with words. Each new word is conditioned upon previous words as well as the grammatical function of the new word, as decided by the grammar-model.

Language models are a staple in many domains including speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction. A dominant technology in language modeling is n-gram models, which are straightforward to construct except for the issue of smoothing, a term that describes a number of different techniques used to better estimate probabilities when the data is too sparse \cite{chen-smoothing}. Some applications of different smoothing techniques are discussed in regard to our models.

\end{document}